{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week2: TensorFlow and Keras Model Training\n",
    "\n",
    "This notebook serves as part of the **Computer Intelligence and its Applications in Mechatronics** course at **Amirkabir University of Technology**. In this notebook, we will demonstrate how to create, train, and evaluate neural network models using TensorFlow and Keras. We will cover both the Sequential and Functional API approaches for building models. Additionally, we will implement a custom training loop using TensorFlow's GradientTape.\n",
    "\n",
    "### Steps Covered:\n",
    "\n",
    "1. **Importing Libraries**: We import necessary libraries including TensorFlow, Keras, and other utility libraries for data preprocessing and visualization.\n",
    "2. **Data Preparation**: We generate a synthetic multi-class dataset, split it into training and testing sets, and standardize the features. The labels are converted to categorical format.\n",
    "3. **Sequential Model Implementation**: We build a neural network using Keras' Sequential API, compile it, and train it on the prepared dataset. The model's performance is evaluated on the test set.\n",
    "4. **Functional Model Implementation**: We build the same neural network using Keras' Functional API, compile it, and train it similarly. The model's performance is evaluated on the test set.\n",
    "5. **Custom Training Loop**: We implement a custom training loop using TensorFlow's GradientTape to manually compute gradients and update model weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "In this section, we import the necessary libraries for our neural network model training and evaluation. These include:\n",
    "\n",
    "- `numpy` for numerical operations.\n",
    "- `matplotlib.pyplot` for plotting and visualization.\n",
    "- `tensorflow` and `keras` for building and training neural network models.\n",
    "- `sklearn` for data preprocessing and evaluation metrics.\n",
    "- `IPython.display` for clearing output in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callback for Plotting Losses\n",
    "\n",
    "In this section, we define a custom callback class `PlotLosses` that inherits from `keras.callbacks.Callback`. This callback will be used to plot the training and validation losses at the end of each epoch. The `on_train_begin` method initializes the variables needed for plotting, and the `on_epoch_end` method updates the plot with the latest loss values. This visualization helps in monitoring the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure()\n",
    "\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Losses')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In this section, we generate a synthetic multi-class dataset using `make_classification` from `sklearn.datasets`. The dataset is then split into training and testing sets. We standardize the features using `StandardScaler` and convert the labels to categorical format using `keras.utils.to_categorical`. This prepares the data for training our neural network models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a multi-class dataset\n",
    "X, y = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n",
    "                           n_classes=3, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model Implementation\n",
    "\n",
    "In this section, we implement a neural network using Keras' Sequential API. The model consists of an input layer, several hidden layers with ReLU activation, and an output layer with softmax activation for multi-class classification. We compile the model with the Adam optimizer and categorical cross-entropy loss. The model is then trained on the prepared dataset, and its performance is evaluated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sequential Model Implementation ###\n",
    "sequential_model = Sequential([\n",
    "    Input(shape=(4,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(5, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Sequential Model\n",
    "\n",
    "In this section, we compile and train the sequential model defined in the previous cell. We use the Adam optimizer and categorical cross-entropy loss. After training, we evaluate the model's performance on the test set and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "sequential_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = sequential_model.fit(X_train, y_train_cat, epochs=200, batch_size=16,\n",
    "                                validation_data=(X_test, y_test_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = sequential_model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Functional Model Implementation\n",
    "\n",
    "In this section, we implement a neural network using Keras' Functional API. The model consists of an input layer, several hidden layers with ReLU activation, and an output layer with softmax activation for multi-class classification. We compile the model with the Adam optimizer and categorical cross-entropy loss. The model is then trained on the prepared dataset, and its performance is evaluated on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functional Model Implementation ###\n",
    "inputs = Input(shape=(4,))\n",
    "x = Dense(16, activation='relu')(inputs)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "functional_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "functional_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "functional_model.fit(X_train, y_train_cat, epochs=200, batch_size=16,\n",
    "                                validation_data=(X_test, y_test_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = functional_model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Custom Training Loop\n",
    "\n",
    "In this section, we implement a custom training loop using TensorFlow's `GradientTape`. This approach allows us to manually compute gradients and update model weights. We use the Adam optimizer and categorical cross-entropy loss. The training loop runs for a specified number of epochs, and the loss is printed every 10 epochs to monitor the training progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = sequential_model(X_train, training=True)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y_train_cat, predictions)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    grads = tape.gradient(loss, sequential_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, sequential_model.trainable_variables))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
